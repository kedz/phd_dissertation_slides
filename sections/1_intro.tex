\begin{frame}{Salience Estimation and Faithful Generation}

\begin{itemize}

    \item Automatic Summarization is an old dream of computer scientists (Luhn, 1958; Edmundson, 1969) as is Natural Language Generation (NLG) (Yngve, 1961)

\vspace{20pt}

\item Summarization researchers and NLG researchers
    (with some notable exceptions) have been focused on different goals.

\begin{itemize}

  \item Summarization: focused on identifying salient information.
  \item NLG: interested in syntactic/semantic representations for
        planning and realizing natural language utterances.

\end{itemize}

\vspace{20pt}

\item \textbf{End-to-End Deep Learning} models of abstractive summarization
    have emerged as a very successful paradigm for performing single document
    summarization (largely independently of these research communities).
        
\end{itemize}
\end{frame}

\begin{frame}[t]{Salience Estimation and Faithful Generation}

\begin{itemize} 
        
\item \textbf{(End-to-End Deep Learning Paradigm)} A Seq2Seq model is trained to map input text to output text directly.
        
        \begin{center}
         \resizebox{0.45\textwidth}{!}{\begin{tikzpicture}
        \node[draw] (doc) at (0,0) {Document}; 
        \node[draw,dashed,align=center] (model) at (3,0) {Seq2Seq \\ Model};
        \node[draw] (sum) at (6,0) {Summary};
        \draw[->,line width=0.5mm] (doc) -- (model);
        \draw[->,line width=0.5mm] (model) -- (sum);
        %\uncover<2->{\node[] (tag) at (4,-0.75) {Whats happening in here?};}
\end{tikzpicture}}
    \end{center}

\only<1-2>{
    \uncover<2->{
    \vspace{10pt}

\item Requires lots of document/summary pairs -- not available for most  domains.

    \vspace{10pt}
\item Difficult to understand why a particular summary was generated.

\begin{itemize}
\item Models may be exploiting dataset specific artifacts. (Kry\'sci\'nski et al., 2019)

    \vspace{5pt}

\item Models often hallucinate information/misrepresent the input.
    (Maynez et al., 2020)
    
    \vspace{5pt}

\item Difficult to attribute errors to failures of the language model or
    lack of world knowledge. (Wiseman et al., 2017) 
\end{itemize}

}}

\only<3->{

\item Without explicit justifications for 
    \begin{itemize}
        \item  why particular content is important, and
        \item how that information is to be organized  
   \end{itemize}

      \vspace{10pt}
   \item it seems unlikely that end-to-end abstractive models will be able to address interesting queries like:

\begin{itemize}
  \item Summarize a transcript of a doctor's appointment, focusing on evidence
      for a particular diagnosis (that is medically sound).
  \item Summarize last week's Covid-19 news, comparing and contrasting topics
      that were highlighted by news outlets with differing political 
      ideologies.

 \end{itemize}
 \hfill
}
\end{itemize}

\end{frame}

\begin{frame}{Salience Estimation and Faithful Generation}
\begin{itemize}
    
\item In this thesis, we prefer breaking this process up into two steps:

\vspace{10pt}

\begin{center}

\resizebox{0.8\textwidth}{!}{
\begin{tikzpicture}
    \node[draw] (doc) at (0,0) {Document}; 

    \node[draw,align=center,dashed] (ext) at (3,0) {\alert<2>{Extraction} \\ \alert<2>{Model}};
    \node[draw,align=center] (cnt) at (6,0) {Salient \\ Information};
    \node[draw,align=center,dashed] (gen) at (9,0) {\alert<3>{Generation} \\ \alert<3>{Model}};
        \node[draw] (sum) at (12,0) {Summary};
        \draw[->,line width=0.5mm] (doc) -- (ext);
        \draw[->,line width=0.5mm] (ext) -- (cnt);
        \draw[->,line width=0.5mm] (cnt) -- (gen);
        \draw[->,line width=0.5mm] (gen) -- (sum);
        \node (P1) at ($(ext)+(-0.35,-1)$) 
            { $ \underbrace{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}_{\text{What to say.}}$  };
        \node (P2) at ($(gen)+(0.35,-1)$) 
            { $ \underbrace{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}_{\text{How to say it.}}$  };

        \node at ($(P1)+(0,-1.0)$) {Focus of Summarization Researchers};
        \node at ($(P2)+(0,-1.0)$) {Focus of NLG Researchers};

\end{tikzpicture}}
\end{center}
\only<2>{
\item How do learned models of sentence extraction make predictions 
    about sentence salience?
    \begin{itemize}
            \item single document summarization
            \item query focused, stream summarization
            \item neural models and feature based models of salience
    \end{itemize}
  }
  \only<3>{
\item  How to design reliable neural NLG
    models?
    \begin{itemize}
        \item We study this in an idealized setting where we have a formal
            meaning representation of the content to be realized.
        \item We evaluate techniques for training neural NLG models that 
        respect
        the semantics of the input meaning representation and provide some surface level control.
  \end{itemize}
}

\end{itemize}

\end{frame}

\begin{frame}{Contributions}
    \begin{itemize}
        \item Salience Estimation with Deep Learning Content Selection Models
            \begin{itemize}
                \item We perform an analysis of neural models of single document extractive summarization.
                \item We show that models are strongly influenced by  position heuristics, especially for news, and are less sensistive to content-based signals.
            \end{itemize}
            \vspace{5pt}
        \item<2-> Salience Estimation with Structured Content Selection Models 
            \begin{itemize}
                \item  On stream summarization task we demonstrate the importance of content based features for salience estimation
                \item 2 stream summarization models incorporating content-based salience estimates.
            \end{itemize}
            \vspace{10pt}
        \item<3-> Faithful Generation
        \begin{itemize}
            \item Noise-injection sampling and self-training based data augmentation.
        \end{itemize}
            \vspace{5pt}
        \item<4-> Controllable Generation
        \begin{itemize}
        \item Alignment Training for controlling surface realization order in neural NLG models.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Outline}
    \begin{itemize}
        \item[\textbf{Part I.}] \textbf{What to say?}
        \begin{enumerate}
            \item Salience Estimation with Deep Learning Content Selection
                    Models
        \begin{itemize}
            \item single document summarization
            \item neural models
        \end{itemize}
                \vspace{10pt}
            \item Salience Estimation with Structured Content Selection Models 
        \begin{itemize}
            \item news stream summarization
            \item feature-based models
        \end{itemize}
        \end{enumerate}
        \vspace{10pt}
    \item[\textbf{Part II.}] \textbf{How to say it?}
        \begin{enumerate}
            \item[3.] Faithful Generation
                \vspace{10pt}
            \item[4.] Controllable Generation
        \end{enumerate}
    \end{itemize}

\end{frame}
